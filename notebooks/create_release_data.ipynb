{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from itertools import chain \n",
    "import statsmodels\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_versions = [\"v8\", \"v9\", \"v12\", \"v13\"]\n",
    "prod_versions = [\"v14\", \"v15\"]\n",
    "prod_versions = [\"v17\"]\n",
    "annotation_path = \"../annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_format(qas):\n",
    "    return [{\n",
    "        \"qa_id\": qa[\"questionId\"],\n",
    "        \"sent_id\": qa[\"sentId\"],\n",
    "        \"predicate_idx\": int(qa[\"predicateId\"].split(\"-\")[0]),\n",
    "        \"predicate_pos\": qa[\"predicatePos\"],\n",
    "        \"predicate\": qa[\"predicate\"],\n",
    "        \"question\": qa[\"question\"],\n",
    "        \"answer_idx\": qa[\"answerId\"],\n",
    "        \"answer\":  qa[\"answer\"]\n",
    "    } for qa in qas]\n",
    "\n",
    "def get_span_format(spans):\n",
    "    return [{\n",
    "        \"id\": span[\"id\"],\n",
    "        \"start\": span[\"start\"],\n",
    "        \"end\": span[\"end\"],\n",
    "        \"qaIds\": span[\"qaIds\"]\n",
    "    } for span in spans if not span[\"predicate\"] and not span[\"include_predicate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstructure of sample:\\n\\n{\\n    \"source\": \"\",\\n    \"summary\": [[]] # list of sentences\\n    \"qas\": [\\n        \"qa_id\": 0\\n        \"sent_id\": 0\\n        \"predicate_idx\": \"\",\\n        \"answer_idx\": [],\\n        \"question\": \"\",\\n        \"answer\":  \"\",\\n        \"model\": \"\"\\n        \"consistent\": [True, True, True],\\n    ],\\n    \"sourceId\": 0,\\n    \"token_labels\": [[]], \\n    \"sentence_labels\": [],\\n    \"dataset\": \"cliff\",\\n    \"origin\": \"xsum\",\\n    \"model\": \"bart\"\\n}\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "structure of sample:\n",
    "\n",
    "{\n",
    "    \"source\": \"\",\n",
    "    \"summary\": [[]] # list of sentences\n",
    "    \"qas\": [\n",
    "        \"qa_id\": 0\n",
    "        \"sent_id\": 0\n",
    "        \"predicate_idx\": \"\",\n",
    "        \"answer_idx\": [],\n",
    "        \"question\": \"\",\n",
    "        \"answer\":  \"\",\n",
    "        \"model\": \"\"\n",
    "        \"consistent\": [True, True, True],\n",
    "    ],\n",
    "    \"sourceId\": 0,\n",
    "    \"token_labels\": [[]], \n",
    "    \"sentence_labels\": [],\n",
    "    \"dataset\": \"cliff\",\n",
    "    \"origin\": \"xsum\",\n",
    "    \"model\": \"bart\"\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngo through the prod annotations \\nbuild the sructure \\nread the original file to add missing fields\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "go through the prod annotations \n",
    "build the sructure \n",
    "read the original file to add missing fields\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load cliff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation(summary):\n",
    "    df_spans = pd.DataFrame([x for x in summary[\"spans\"] if \"label\" in x])\n",
    "    df_spans[\"label\"] = df_spans[\"label\"].astype(int)\n",
    "    df_wrong_spans = df_spans[df_spans[\"label\"] == 0]\n",
    "    wrong_qas = set(df_wrong_spans.explode(\"qaIds\")[\"qaIds\"].unique())\n",
    "\n",
    "    df_qas = pd.DataFrame(summary[\"qas\"])\n",
    "    df_qas.loc[df_qas[\"questionId\"].isin(wrong_qas), \"label\"] = 1\n",
    "    df_qas[\"label\"] = df_qas[\"label\"].astype(int)\n",
    "    return df_qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-388107_Shadow foreign policy 0 17 1\n",
      "CNN-388107_Shadow foreign policy 0 18 1\n",
      "CNN-388107_Shadow foreign policy 1 19 1\n",
      "CNN-388107_Shadow foreign policy 1 20 1\n",
      "CNN-340132_Trump administration's peace plan 0 22 1\n",
      "CNN-340132_Trump administration's peace plan 0 23 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/loc-unfaith/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m qa_annotations \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m annot \u001b[38;5;129;01min\u001b[39;00m annotations:\n\u001b[0;32m---> 56\u001b[0m     df_qas \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannot\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummaries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row_id, qa \u001b[38;5;129;01min\u001b[39;00m df_qas\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# for qa in annot[\"summaries\"][i][\"qas\"]:\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         qa_id \u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestionId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mprocess_annotation\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_annotation\u001b[39m(summary):\n\u001b[1;32m      2\u001b[0m     df_spans \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspans\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[0;32m----> 3\u001b[0m     df_spans[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_spans\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      4\u001b[0m     df_wrong_spans \u001b[38;5;241m=\u001b[39m df_spans[df_spans[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     wrong_qas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(df_wrong_spans\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqaIds\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqaIds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n",
      "File \u001b[0;32m~/anaconda3/envs/loc-unfaith/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/loc-unfaith/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "released_data = []\n",
    "for version in prod_versions:\n",
    "    dir_path = os.path.join(annotation_path, version)\n",
    "    for source_id in os.listdir(dir_path):\n",
    "        if not os.path.isdir(os.path.join(dir_path, source_id)):\n",
    "            continue\n",
    "\n",
    "        # load input file to mturk \n",
    "        # origin_path = f\"../src/data/cliff_multiple_summaries/{source_id}.json\"\n",
    "        # origin_path = f\"../src/data/factscore_filtered/{source_id}.json\"\n",
    "        origin_path = f\"../src/data/tofueval_chosen/mediasum/{source_id}.json\"\n",
    "        with open(origin_path, \"r\") as f:\n",
    "            origin = json.load(f)\n",
    "\n",
    "        annotations = []\n",
    "        for annotator in os.listdir(os.path.join(dir_path, source_id)):\n",
    "            annotator_path = os.path.join(dir_path, source_id, annotator)\n",
    "            with open(annotator_path, \"r\") as f:\n",
    "                annotator = json.load(f)\n",
    "            annotations.append(annotator)\n",
    "\n",
    "        if len(annotations) != 3:\n",
    "            continue\n",
    "        \n",
    "        for i, summary in enumerate(origin[\"summaries\"]):\n",
    "            obj = {\n",
    "                \"source_id\": origin[\"sourceId\"],\n",
    "                \"source\": [x[\"text\"] for x in origin[\"source\"]],\n",
    "                \"summary\": [[x[\"text\"] for x in summary[\"tokens\"]]],\n",
    "                # \"spans\": get_span_format(summary[\"spans\"]),\n",
    "                # \"qas\": get_qa_format(summary[\"qas\"]),\n",
    "                # \"cliff_labels\": [[x[\"label\"] for x in summary[\"tokens\"]]], \n",
    "                \"model\": summary[\"summaryId\"],\n",
    "                \"datasource\": origin[\"datasource\"],\n",
    "                \"dataset\": origin[\"dataset\"],\n",
    "            }\n",
    "\n",
    "            obj[\"qas\"] = get_qa_format(annotations[0][\"summaries\"][i][\"qas\"])\n",
    "            obj[\"spans\"] = get_span_format(annotations[0][\"summaries\"][i][\"spans\"])\n",
    "            \n",
    "\n",
    "            \n",
    "            span_annotations = collections.defaultdict(list)\n",
    "            for annot in annotations:\n",
    "                for span in annot[\"summaries\"][i][\"spans\"]:\n",
    "                    span_id = span[\"id\"]\n",
    "                    if \"label\" in span:\n",
    "                        span_label = span[\"label\"]\n",
    "                        span_annotations[span_id].append(span_label)\n",
    "\n",
    "            for j, span in enumerate(obj[\"spans\"]):\n",
    "                obj[\"spans\"][j][\"annotations\"] = span_annotations[span[\"id\"]]\n",
    "\n",
    "            qa_annotations = collections.defaultdict(list)\n",
    "            for annot in annotations:\n",
    "                df_qas = process_annotation(annot[\"summaries\"][i])\n",
    "                for row_id, qa in df_qas.iterrows():\n",
    "                # for qa in annot[\"summaries\"][i][\"qas\"]:\n",
    "                    qa_id = qa[\"questionId\"]\n",
    "                    qa_label = qa[\"label\"]  # 0 if \"label\" in qa and qa[\"label\"] == 0 else 1\n",
    "                    qa_annotations[qa_id].append(qa_label)\n",
    "\n",
    "            for j, qa in enumerate(obj[\"qas\"]):\n",
    "                if len(qa_annotations[qa[\"qa_id\"]]) != 3:\n",
    "                    print(source_id, i, j, len(qa_annotations[qa[\"qa_id\"]]))\n",
    "                obj[\"qas\"][j][\"annotations\"] = qa_annotations[qa[\"qa_id\"]]\n",
    "\n",
    "            released_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>model</th>\n",
       "      <th>datasource</th>\n",
       "      <th>dataset</th>\n",
       "      <th>qas</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-25553_Airlines' commitment to service impr...</td>\n",
       "      <td>[CAROL, LIN, ,, CNN, ANCHOR, :, Well, ,, the, ...</td>\n",
       "      <td>[[The, airlines, promised, service, improvemen...</td>\n",
       "      <td>Model-Extra</td>\n",
       "      <td>mediasum</td>\n",
       "      <td>tofueval</td>\n",
       "      <td>[{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 2...</td>\n",
       "      <td>[{'id': 0, 'start': 1, 'end': 2, 'qaIds': [0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN-25553_Airlines' commitment to service impr...</td>\n",
       "      <td>[CAROL, LIN, ,, CNN, ANCHOR, :, Well, ,, the, ...</td>\n",
       "      <td>[[Airlines, ', commitment, to, service, improv...</td>\n",
       "      <td>B</td>\n",
       "      <td>mediasum</td>\n",
       "      <td>tofueval</td>\n",
       "      <td>[{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...</td>\n",
       "      <td>[{'id': 0, 'start': 0, 'end': 1, 'qaIds': [4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN-164885_Cuban celebration and government ga...</td>\n",
       "      <td>[FEYERICK, :, We, 'll, get, to, Donald, Trump,...</td>\n",
       "      <td>[[Cuba, celebrated, 50, years, since, defeatin...</td>\n",
       "      <td>Model-Extra</td>\n",
       "      <td>mediasum</td>\n",
       "      <td>tofueval</td>\n",
       "      <td>[{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...</td>\n",
       "      <td>[{'id': 0, 'start': 0, 'end': 1, 'qaIds': [0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN-164885_Cuban celebration and government ga...</td>\n",
       "      <td>[FEYERICK, :, We, 'll, get, to, Donald, Trump,...</td>\n",
       "      <td>[[Cubans, celebrated, the, 50th, anniversary, ...</td>\n",
       "      <td>E</td>\n",
       "      <td>mediasum</td>\n",
       "      <td>tofueval</td>\n",
       "      <td>[{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...</td>\n",
       "      <td>[{'id': 0, 'start': 0, 'end': 1, 'qaIds': [0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-7972_Federal Reserve's policy</td>\n",
       "      <td>[DEBORAH, MARCHINI, ,, CNN, ANCHOR, :, Where, ...</td>\n",
       "      <td>[[Sandy, Weill, expects, another, Fed, rate, h...</td>\n",
       "      <td>Model-Extra</td>\n",
       "      <td>mediasum</td>\n",
       "      <td>tofueval</td>\n",
       "      <td>[{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 2...</td>\n",
       "      <td>[{'id': 0, 'start': 0, 'end': 2, 'qaIds': [0],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           source_id  \\\n",
       "0  CNN-25553_Airlines' commitment to service impr...   \n",
       "1  CNN-25553_Airlines' commitment to service impr...   \n",
       "2  CNN-164885_Cuban celebration and government ga...   \n",
       "3  CNN-164885_Cuban celebration and government ga...   \n",
       "4                  CNN-7972_Federal Reserve's policy   \n",
       "\n",
       "                                              source  \\\n",
       "0  [CAROL, LIN, ,, CNN, ANCHOR, :, Well, ,, the, ...   \n",
       "1  [CAROL, LIN, ,, CNN, ANCHOR, :, Well, ,, the, ...   \n",
       "2  [FEYERICK, :, We, 'll, get, to, Donald, Trump,...   \n",
       "3  [FEYERICK, :, We, 'll, get, to, Donald, Trump,...   \n",
       "4  [DEBORAH, MARCHINI, ,, CNN, ANCHOR, :, Where, ...   \n",
       "\n",
       "                                             summary        model datasource  \\\n",
       "0  [[The, airlines, promised, service, improvemen...  Model-Extra   mediasum   \n",
       "1  [[Airlines, ', commitment, to, service, improv...            B   mediasum   \n",
       "2  [[Cuba, celebrated, 50, years, since, defeatin...  Model-Extra   mediasum   \n",
       "3  [[Cubans, celebrated, the, 50th, anniversary, ...            E   mediasum   \n",
       "4  [[Sandy, Weill, expects, another, Fed, rate, h...  Model-Extra   mediasum   \n",
       "\n",
       "    dataset                                                qas  \\\n",
       "0  tofueval  [{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 2...   \n",
       "1  tofueval  [{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...   \n",
       "2  tofueval  [{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...   \n",
       "3  tofueval  [{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 1...   \n",
       "4  tofueval  [{'qa_id': 0, 'sent_id': 0, 'predicate_idx': 2...   \n",
       "\n",
       "                                               spans  \n",
       "0  [{'id': 0, 'start': 1, 'end': 2, 'qaIds': [0, ...  \n",
       "1  [{'id': 0, 'start': 0, 'end': 1, 'qaIds': [4, ...  \n",
       "2  [{'id': 0, 'start': 0, 'end': 1, 'qaIds': [0, ...  \n",
       "3  [{'id': 0, 'start': 0, 'end': 1, 'qaIds': [0, ...  \n",
       "4  [{'id': 0, 'start': 0, 'end': 2, 'qaIds': [0],...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(released_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"../annotations/prod/cliff.jsonl\"\n",
    "output_path = \"../annotations/prod/factscore.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(output_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annotations\"] = df[\"qas\"].apply(lambda lst: [x[\"annotations\"] for x in lst])\n",
    "all_qas = [x for x in list(chain.from_iterable(df[\"annotations\"])) if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for x in all_qas if len(x) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/yrc5nyls2cn1fp3nhbcmvg040000gn/T/ipykernel_48845/2090474237.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(all_qas)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]), list([1, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 1, 0]), list([1, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 1]), list([0, 1, 0]), list([0, 0, 0]),\n",
       "       list([0, 1, 0]), list([0, 1, 1]), list([0, 0, 0]), list([0]),\n",
       "       list([0]), list([1]), list([0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([1, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0]), list([0]), list([1]), list([1]), list([0]), list([0]),\n",
       "       list([0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([1, 0, 0]), list([0]),\n",
       "       list([0]), list([0, 0, 0]), list([0, 0, 0]), list([1, 1, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([1, 0, 0]), list([1, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 1]), list([0, 0, 1]), list([0, 1, 1]), list([0, 0, 0]),\n",
       "       list([0]), list([0]), list([0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 1, 1]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]),\n",
       "       list([1, 1, 0]), list([0, 1, 0]), list([0, 1, 1]), list([0, 1, 1]),\n",
       "       list([0, 1, 1]), list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 1, 0]), list([0, 1, 0]), list([0, 0, 0]), list([0, 1, 1]),\n",
       "       list([1, 0, 0]), list([1, 0, 0]), list([0, 1, 0]), list([1, 0, 0]),\n",
       "       list([1]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([1, 0, 0]),\n",
       "       list([1, 0, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 1]),\n",
       "       list([0, 1, 1]), list([0, 0, 1]), list([0, 1, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]),\n",
       "       list([0, 0, 0]), list([1, 0, 1]), list([0, 1, 0]), list([0, 0, 0]),\n",
       "       list([1, 1, 1]), list([1, 0, 1]), list([0, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 1]), list([1, 0, 1]), list([0, 0, 1]), list([1, 1, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 1]), list([0, 1, 0]),\n",
       "       list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([1, 0, 0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0]), list([0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]), list([0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 1, 1]), list([0, 0, 0]), list([0, 0, 0]), list([1, 0, 0]),\n",
       "       list([1, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0]),\n",
       "       list([0]), list([0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0]), list([0]), list([1, 1, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([1, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([1, 0, 0]), list([0, 0, 0]), list([0, 1, 1]),\n",
       "       list([1, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0]), list([0, 1, 0]), list([0, 1, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0]),\n",
       "       list([0]), list([0, 1, 1]), list([0, 1, 1]), list([0, 1, 1]),\n",
       "       list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([1, 1, 1]), list([1, 1, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([1]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 1]), list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([1, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0]), list([1]), list([1]), list([0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 1, 1]), list([0, 1, 1]), list([1, 1, 1]),\n",
       "       list([1, 1, 1]), list([1, 1, 1]), list([1, 1, 1]), list([1, 1, 1]),\n",
       "       list([1, 1, 1]), list([1, 1, 1]), list([0, 0, 0]), list([1, 0, 0]),\n",
       "       list([0, 1, 1]), list([0, 1, 1]), list([1, 1, 1]), list([1, 1, 1]),\n",
       "       list([1, 1, 1]), list([1, 1, 1]), list([1, 1, 1]), list([1, 0, 0]),\n",
       "       list([1, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 1, 0]), list([0, 0, 0]), list([0]), list([0]),\n",
       "       list([0, 0, 0]), list([0, 1, 0]), list([0, 0, 0]), list([0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 0]), list([0, 1, 0]),\n",
       "       list([0, 1, 0]), list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 1, 1]), list([0, 1, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 1]), list([0, 0, 0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0]), list([0]), list([1, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([1, 0, 0]), list([1, 0, 1]), list([0, 1, 1]),\n",
       "       list([0, 0, 1]), list([0, 0, 1]), list([1, 0, 1]), list([1, 0, 0]),\n",
       "       list([1, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0]), list([0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0]), list([1]), list([0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 1]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([1, 0, 0]), list([0, 0, 0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0]), list([0]), list([0]), list([0]),\n",
       "       list([0]), list([1]), list([0]), list([0, 1, 1]), list([0, 0, 0]),\n",
       "       list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 1]), list([0, 0, 1]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0]), list([0]), list([0]), list([1, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 1, 1]), list([1, 1, 1]), list([1, 1, 1]),\n",
       "       list([1, 1, 1]), list([1, 1, 1]), list([1, 0, 0]), list([1]),\n",
       "       list([0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 1, 1]), list([1, 0, 0]),\n",
       "       list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 0]), list([0, 0, 1]),\n",
       "       list([0])], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/yrc5nyls2cn1fp3nhbcmvg040000gn/T/ipykernel_48845/320864670.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  table = statsmodels.stats.inter_rater.aggregate_raters(np.array(all_qas))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object of too small depth for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mstatsmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minter_rater\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_raters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_qas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m fleiss \u001b[38;5;241m=\u001b[39m statsmodels\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39minter_rater\u001b[38;5;241m.\u001b[39mfleiss_kappa(table[\u001b[38;5;241m0\u001b[39m], method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfleiss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m fleiss\n",
      "File \u001b[0;32m~/anaconda3/envs/loc-unfaith/lib/python3.8/site-packages/statsmodels/stats/inter_rater.py:129\u001b[0m, in \u001b[0;36maggregate_raters\u001b[0;34m(data, n_cat)\u001b[0m\n\u001b[1;32m    127\u001b[0m tt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_rows, n_cat), \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_):\n\u001b[0;32m--> 129\u001b[0m     ro \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     tt[idx, :\u001b[38;5;28mlen\u001b[39m(ro)] \u001b[38;5;241m=\u001b[39m ro\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tt, cat_uni\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object of too small depth for desired array"
     ]
    }
   ],
   "source": [
    "table = statsmodels.stats.inter_rater.aggregate_raters(np.array(all_qas))\n",
    "fleiss = statsmodels.stats.inter_rater.fleiss_kappa(table[0], method='fleiss')\n",
    "fleiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2805"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstructure of sample:\\n\\n{\\n    \"source\": [],\\n    \"summary\": [[]] # list of sentences\\n    \"qas\": [\\n        \"qa_id\": 0\\n        \"sent_id\": 0\\n        \"predicate_idx\": \"\",\\n        \"answer_idx\": [],\\n        \"question\": \"\",\\n        \"answer\":  \"\",\\n        \"model\": \"\"\\n        \"consistent\": [True, True, True],\\n    ],\\n    \"sourceId\": 0,\\n    \"token_labels\": [[]], \\n    \"sentence_labels\": [],\\n    \"dataset\": \"cliff\",\\n    \"origin\": \"xsum\",\\n    \"model\": \"bart\"\\n}\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "structure of sample:\n",
    "\n",
    "{\n",
    "    \"source\": [],\n",
    "    \"summary\": [[]] # list of sentences\n",
    "    \"qas\": [\n",
    "        \"qa_id\": 0\n",
    "        \"sent_id\": 0\n",
    "        \"predicate_idx\": \"\",\n",
    "        \"answer_idx\": [],\n",
    "        \"question\": \"\",\n",
    "        \"answer\":  \"\",\n",
    "        \"model\": \"\"\n",
    "        \"consistent\": [True, True, True],\n",
    "    ],\n",
    "    \"sourceId\": 0,\n",
    "    \"token_labels\": [[]], \n",
    "    \"sentence_labels\": [],\n",
    "    \"dataset\": \"cliff\",\n",
    "    \"origin\": \"xsum\",\n",
    "    \"model\": \"bart\"\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "    \"source_id\": origin[\"sourceId\"],\n",
    "    \"source\": origin[\"source\"],\n",
    "    \"summary\": [[x[\"text\"] for x in origin[\"summaries\"][0][\"tokens\"]]],\n",
    "    \"spans\": get_span_format(origin[\"summaries\"][0][\"spans\"]),\n",
    "    \"qas\": get_qa_format(origin[\"summaries\"][0][\"qas\"]),\n",
    "    \"cliff_labels\": [[x[\"label\"] for x in origin[\"summaries\"][0][\"tokens\"]]], \n",
    "    \"model\": origin[\"summaries\"][0][\"summaryId\"],\n",
    "    \"datasource\": origin[\"datasource\"],\n",
    "    \"dataset\": origin[\"dataset\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'start': 1,\n",
       "  'end': 3,\n",
       "  'qaIds': [8, 13, 17, 0, 4],\n",
       "  'annotations': [1, 1, 1]},\n",
       " {'id': 1,\n",
       "  'start': 1,\n",
       "  'end': 6,\n",
       "  'qaIds': [9, 14, 1, 5],\n",
       "  'annotations': [1, 1, 1]},\n",
       " {'id': 2, 'start': 4, 'end': 5, 'qaIds': [18], 'annotations': [1, 1, 1]},\n",
       " {'id': 6, 'start': 10, 'end': 12, 'qaIds': [6], 'annotations': [1, 1, 1]},\n",
       " {'id': 10,\n",
       "  'start': 14,\n",
       "  'end': 15,\n",
       "  'qaIds': [11, 19],\n",
       "  'annotations': [1, 1, 1]},\n",
       " {'id': 12, 'start': 20, 'end': 22, 'qaIds': [12], 'annotations': [1, 1, 1]}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[\"spans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_annotations = collections.defaultdict(list)\n",
    "for annot in annotations:\n",
    "    for span in annot[\"summaries\"][0][\"spans\"]:\n",
    "        span_id = span[\"id\"]\n",
    "        if \"label\" in span:\n",
    "            span_label = span[\"label\"]\n",
    "            span_annotations[span_id].append(span_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_annotations = collections.defaultdict(list)\n",
    "for annot in annotations:\n",
    "    for qa in annot[\"summaries\"][0][\"qas\"]:\n",
    "        qa_id = qa[\"questionId\"]\n",
    "        qa_label = 1 if \"label\" in qa and qa[\"label\"] == 0 else 0\n",
    "        qa_annotations[qa_id].append(qa_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [1, 1, 1],\n",
       "             1: [1, 1, 1],\n",
       "             2: [1, 1, 1],\n",
       "             3: [1, 0, 1],\n",
       "             4: [1, 1, 1],\n",
       "             5: [1, 1, 1],\n",
       "             6: [1, 1, 1],\n",
       "             7: [1, 1, 1],\n",
       "             8: [1, 1, 1],\n",
       "             9: [1, 1, 1],\n",
       "             10: [1, 1, 1],\n",
       "             11: [1, 1, 1],\n",
       "             12: [1, 1, 1],\n",
       "             13: [1, 1, 1],\n",
       "             14: [1, 1, 1],\n",
       "             15: [1, 1, 1],\n",
       "             16: [1, 1, 1],\n",
       "             17: [1, 1, 1],\n",
       "             18: [1, 1, 1],\n",
       "             19: [1, 1, 1]})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, span in enumerate(obj[\"spans\"]):\n",
    "    obj[\"spans\"][i][\"annotations\"] = span_annotations[span[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-unfaith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
